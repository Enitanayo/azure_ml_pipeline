{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f8b02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d45f3",
   "metadata": {},
   "source": [
    "### PHASE 1.1 - CONNECT TO YOUR WORKSPACE VIA SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f04583f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current LoggerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "# establish connection to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id,\n",
    "    resource_group,\n",
    "    workspace_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea905e2",
   "metadata": {},
   "source": [
    "### CREATE AND REGISTER YOUR ENVRONMENT WITH YAML FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12b0eb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'pipeline_env', 'description': 'Custom environment using Docker image and Conda YAML.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/9c16564d-8303-4672-9a52-2b4ba14a38d6/resourceGroups/azure_ml_pipeline/providers/Microsoft.MachineLearningServices/workspaces/azure_ml_workspace/environments/pipeline_env/versions/1', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\Administrator\\\\Desktop\\\\EDUCATION\\\\COURSES\\\\Projects\\\\ML Projects\\\\azure_ml_pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000011AFCE65390>, 'serialize': <msrest.serialization.Serializer object at 0x0000011AF9E9BE90>, 'version': '1', 'conda_file': {'channels': ['defaults', 'conda-forge'], 'dependencies': ['python=3.8', 'pip', {'pip': ['numpy', 'pandas', 'scikit-learn', 'azure-cli', 'azure-ai-ml', 'azure-identity', 'python-dotenv', 'jupyter']}], 'name': 'pipeline_env'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"defaults\",\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.8\",\\n    \"pip\",\\n    {\\n      \"pip\": [\\n        \"numpy\",\\n        \"pandas\",\\n        \"scikit-learn\",\\n        \"azure-cli\",\\n        \"azure-ai-ml\",\\n        \"azure-identity\",\\n        \"python-dotenv\",\\n        \"jupyter\"\\n      ]\\n    }\\n  ],\\n  \"name\": \"pipeline_env\"\\n}'})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "# Create custom environment using Docker base image + Conda YAML\n",
    "# Conda yaml can be found in the conda-yamls directory\n",
    "custom_env = Environment(\n",
    "    name=\"pipeline_env\",\n",
    "    description=\"Custom environment using Docker image and Conda YAML.\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "    conda_file=\"conda-yamls/pipeline_env.yml\"  \n",
    ")\n",
    "\n",
    "# Register the environment with Azure ML\n",
    "ml_client.environments.create_or_update(custom_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7237e6",
   "metadata": {},
   "source": [
    "### PHASE 1.2 - UPLOAD YOUR DATA AND REGISTER AS A DATA ASSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa7fd641",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(UserError) A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nCode: UserError\nMessage: A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"8a2eb1a0e06f740baa769129da6df3e6\",\n        \"request\": \"99405f02655e952a\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"eastus\"\n}Type: Location\nInfo: {\n    \"value\": \"eastus\"\n}Type: Time\nInfo: {\n    \"value\": \"2025-06-16T16:03:43.5676075+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"Immutable\",\n        \"innerError\": {\n            \"code\": \"DataVersionPropertyImmutable\",\n            \"innerError\": null\n        }\n    }\n}Type: MessageFormat\nInfo: {\n    \"value\": \"A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's {property} cannot be changed. Only tags, description, and isArchived can be updated.\"\n}Type: MessageParameters\nInfo: {\n    \"value\": {\n        \"property\": \"data uri\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssetTypes\n\u001b[32m      5\u001b[39m my_data = Data(\n\u001b[32m      6\u001b[39m     path=\u001b[33m\"\u001b[39m\u001b[33mdata/visa_application_gen.csv\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# path to your local file\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mtype\u001b[39m=AssetTypes.URI_FILE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     version=\u001b[33m\"\u001b[39m\u001b[33m1.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\EDUCATION\\COURSES\\Projects\\ML Projects\\azure_ml_pipeline\\venv\\Lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:288\u001b[39m, in \u001b[36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(ACTIVITY_SPAN):\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[32m    286\u001b[39m             logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[32m    287\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[33m\"\u001b[39m\u001b[33mpackage_logger\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\EDUCATION\\COURSES\\Projects\\ML Projects\\azure_ml_pipeline\\venv\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_data_operations.py:425\u001b[39m, in \u001b[36mDataOperations.create_or_update\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ex) == ASSET_PATH_ERROR:\n\u001b[32m    419\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m AssetPathException(\n\u001b[32m    420\u001b[39m             message=CHANGED_ASSET_PATH_MSG,\n\u001b[32m    421\u001b[39m             tartget=ErrorTarget.DATA,\n\u001b[32m    422\u001b[39m             no_personal_data_message=CHANGED_ASSET_PATH_MSG_NO_PERSONAL_DATA,\n\u001b[32m    423\u001b[39m             error_category=ErrorCategory.USER_ERROR,\n\u001b[32m    424\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\EDUCATION\\COURSES\\Projects\\ML Projects\\azure_ml_pipeline\\venv\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_data_operations.py:400\u001b[39m, in \u001b[36mDataOperations.create_or_update\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    381\u001b[39m     result = _create_or_update_autoincrement(\n\u001b[32m    382\u001b[39m         name=data.name,\n\u001b[32m    383\u001b[39m         body=data_version_resource,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m         **\u001b[38;5;28mself\u001b[39m._init_kwargs,\n\u001b[32m    389\u001b[39m     )\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    391\u001b[39m     result = (\n\u001b[32m    392\u001b[39m         \u001b[38;5;28mself\u001b[39m._operation.begin_create_or_update(\n\u001b[32m    393\u001b[39m             name=name,\n\u001b[32m    394\u001b[39m             version=version,\n\u001b[32m    395\u001b[39m             registry_name=\u001b[38;5;28mself\u001b[39m._registry_name,\n\u001b[32m    396\u001b[39m             body=data_version_resource,\n\u001b[32m    397\u001b[39m             **\u001b[38;5;28mself\u001b[39m._scope_kwargs,\n\u001b[32m    398\u001b[39m         ).result()\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._registry_name\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_operation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_version_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scope_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     )\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._registry_name:\n\u001b[32m    410\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get(name=name, version=version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\EDUCATION\\COURSES\\Projects\\ML Projects\\azure_ml_pipeline\\venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\Desktop\\EDUCATION\\COURSES\\Projects\\ML Projects\\azure_ml_pipeline\\venv\\Lib\\site-packages\\azure\\ai\\ml\\_restclient\\v2023_04_01_preview\\operations\\_data_versions_operations.py:566\u001b[39m, in \u001b[36mDataVersionsOperations.create_or_update\u001b[39m\u001b[34m(self, resource_group_name, workspace_name, name, version, body, **kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     map_error(status_code=response.status_code, response=response, error_map=error_map)\n\u001b[32m    565\u001b[39m     error = \u001b[38;5;28mself\u001b[39m._deserialize.failsafe_deserialize(_models.ErrorResponse, pipeline_response)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response, model=error, error_format=ARMErrorFormat)\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m    569\u001b[39m     deserialized = \u001b[38;5;28mself\u001b[39m._deserialize(\u001b[33m'\u001b[39m\u001b[33mDataVersionBase\u001b[39m\u001b[33m'\u001b[39m, pipeline_response)\n",
      "\u001b[31mHttpResponseError\u001b[39m: (UserError) A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nCode: UserError\nMessage: A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"8a2eb1a0e06f740baa769129da6df3e6\",\n        \"request\": \"99405f02655e952a\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"eastus\"\n}Type: Location\nInfo: {\n    \"value\": \"eastus\"\n}Type: Time\nInfo: {\n    \"value\": \"2025-06-16T16:03:43.5676075+00:00\"\n}Type: InnerError\nInfo: {\n    \"value\": {\n        \"code\": \"Immutable\",\n        \"innerError\": {\n            \"code\": \"DataVersionPropertyImmutable\",\n            \"innerError\": null\n        }\n    }\n}Type: MessageFormat\nInfo: {\n    \"value\": \"A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's {property} cannot be changed. Only tags, description, and isArchived can be updated.\"\n}Type: MessageParameters\nInfo: {\n    \"value\": {\n        \"property\": \"data uri\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "# upload and register data as a data asset\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_data = Data(\n",
    "    path=\"data/visa_application_gen.csv\",  # path to your local file\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Visa risk assessment dataset\",\n",
    "    name=\"visa-data\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69d7bf",
   "metadata": {},
   "source": [
    "### PHASE 1.3 - UPLOAD AND REGISTER YOUR COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f29c029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/preprocess/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/preprocess/preprocess.py\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# ---- Argument parsing ----\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str)\n",
    "parser.add_argument(\"--output_dir\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# country risk score dictionary ----\n",
    "country_risk = {'Nigeria': 3, 'India': 2, 'Ghana': 2, 'Kenya': 2, \n",
    "                       'Germany': 0, 'USA': 0, 'UK': 0, 'Pakistan': 3, \n",
    "                       'South Africa': 1, 'Egypt': 2}\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(args.input_data)\n",
    "    df.drop(\"risk_score\", axis=1, inplace=True)\n",
    "\n",
    "    df[\"country_risk_score\"] = df[\"country\"].map(country_risk)\n",
    "    df.drop(\"country\", axis=1, inplace=True)\n",
    "\n",
    "    df[\"sponsorship\"] = df[\"sponsorship\"].map({\"Yes\": 1, \"No\": 0})\n",
    "    \n",
    "    label = df[\"risk_flag\"].values  # (n_samples,) array\n",
    "\n",
    "    # Drop label and encode the rest\n",
    "    df = df.drop(\"risk_flag\", axis=1)\n",
    "\n",
    "    # Define features\n",
    "    num_features = ['age', 'financial_support','travel_history', 'duration']\n",
    "    cat_features = ['visa_type', 'employment_status', 'purpose']\n",
    "\n",
    "    num_transformer = Pipeline([\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", num_transformer, num_features),\n",
    "        (\"cat\", cat_transformer, cat_features),\n",
    "    ])\n",
    "\n",
    "    preprocessed_data = preprocessor.fit_transform(df)\n",
    "\n",
    "    # ---- Save outputs ----\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    np.save(os.path.join(args.output_dir, \"data.npy\"), preprocessed_data)\n",
    "    np.save(os.path.join(args.output_dir, \"y.npy\"), label)\n",
    "    joblib.dump(preprocessor, os.path.join(args.output_dir, \"preprocessor.pkl\"))\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during preprocessing:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bdfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/preprocess/preprocess.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/preprocess/preprocess.yml\n",
    "name: preprocess\n",
    "display_name: Preprocess Application Data\n",
    "version: 3 # increment this if you have changed this file after submission\n",
    "type: command\n",
    "inputs:\n",
    "  input_data:\n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_dir:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment: azureml:pipeline_env:1\n",
    "command: >-\n",
    "  python preprocess.py \n",
    "  --input_data ${{inputs.input_data}} \n",
    "  --output_dir ${{outputs.output_dir}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35b76b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/split_and_train/split_and_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/split_and_train/split_and_train.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", type=str, help=\"Path to preprocessed folder\")\n",
    "    parser.add_argument(\"--model_output\", type=str, help=\"Path to save model\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    output_dir = args.input_data\n",
    "    # Load preprocessed data\n",
    "    \n",
    "    X = np.load(os.path.join(args.input_data, \"data.npy\"))\n",
    "    y = np.load(os.path.join(args.input_data, \"y.npy\"))\n",
    "\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    os.makedirs(args.model_output, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(args.model_output, \"model.pkl\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233c90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/split_and_train/split_and_train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/split_and_train/split_and_train.yml\n",
    "name: split_and_train\n",
    "display_name: Split and Train Model\n",
    "version: 3 # increment this if you have changed this file after submission\n",
    "type: command\n",
    "inputs:\n",
    "  input_data:\n",
    "    type: uri_folder\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment: azureml:pipeline_env:1  # Replace with your environment\n",
    "command: >-\n",
    "  python split_and_train.py\n",
    "  --input_data ${{inputs.input_data}}\n",
    "  --model_output ${{outputs.model_output}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48624efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6fa143aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading preprocess (0.0 MBs): 100%|##########| 2714/2714 [00:01<00:00, 2215.58it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing component registered: preprocess (v3)\n"
     ]
    }
   ],
   "source": [
    "preprocess_component = load_component(source=\"components/preprocess/preprocess.yml\")\n",
    "registered_preprocess_component = ml_client.components.create_or_update(preprocess_component)\n",
    "print(f\"Preprocessing component registered: {registered_preprocess_component.name} (v{registered_preprocess_component.version})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51c59073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading split_and_train (0.0 MBs): 100%|##########| 1966/1966 [00:01<00:00, 1121.39it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training component registered: split_and_train (v3)\n"
     ]
    }
   ],
   "source": [
    "train_component = load_component(source=\"components/split_and_train/split_and_train.yml\")\n",
    "registered_train_component = ml_client.components.create_or_update(train_component)\n",
    "print(f\"Training component registered: {registered_train_component.name} (v{registered_train_component.version})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e01f6",
   "metadata": {},
   "source": [
    "### PHASE 1.4 - CREATE AND RUN PIPELINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input, dsl\n",
    "\n",
    "# Load the components from registry\n",
    "preprocess_component = ml_client.components.get(name=\"preprocess\", version=\"3\")\n",
    "train_component = ml_client.components.get(name=\"split_and_train\", version=\"3\")\n",
    "\n",
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    compute=\"azure-ml-instance\",\n",
    "    description=\"ML pipeline for data preprocessing and model training\"\n",
    ")\n",
    "def pipeline_func(input_data):\n",
    "    preprocess_job = preprocess_component(\n",
    "        input_data=input_data\n",
    "    )\n",
    "\n",
    "    train_job = train_component(\n",
    "        input_data=preprocess_job.outputs.output_dir,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model_output\": train_job.outputs.model_output\n",
    "    }\n",
    "\n",
    "# Create a pipeline job from the function\n",
    "pipeline_job = pipeline_func(\n",
    "    input_data=Input(type=\"uri_file\", path=my_data.path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e603739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tender_square_3j782llsr7\n",
      "Web View: https://ml.azure.com/runs/tender_square_3j782llsr7?wsid=/subscriptions/9c16564d-8303-4672-9a52-2b4ba14a38d6/resourcegroups/azure_ml_pipeline/workspaces/azure_ml_workspace\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2025-06-16 16:36:33Z] Submitting 1 runs, first five are: 12ed1735:15e0e4a0-8152-43be-85f3-80122ff1c546\n",
      "[2025-06-16 16:36:57Z] Completing processing run id 15e0e4a0-8152-43be-85f3-80122ff1c546.\n",
      "[2025-06-16 16:36:58Z] Submitting 1 runs, first five are: 12c61163:1635b683-4ab5-4c61-8179-4ab9c255da14\n",
      "[2025-06-16 16:37:22Z] Completing processing run id 1635b683-4ab5-4c61-8179-4ab9c255da14.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tender_square_3j782llsr7\n",
      "Web View: https://ml.azure.com/runs/tender_square_3j782llsr7?wsid=/subscriptions/9c16564d-8303-4672-9a52-2b4ba14a38d6/resourcegroups/azure_ml_pipeline/workspaces/azure_ml_workspace\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(pipeline_job)\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7773eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
